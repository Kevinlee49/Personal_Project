{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcEnL1pDJZlv/MxXN1cJeU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kevinlee49/Personal_Project/blob/main/detect_meter_%26_number_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0DOubvpNdZBX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime as dt\n",
        "import time\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "from scipy import ndimage\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import L2\n",
        "from tensorflow.keras.layers import MaxPool2D, Conv2D, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "import matplotlib.pyplot as plt\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")"
      ],
      "metadata": {
        "id": "_3dJAhkrdaxj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decimal_process(b, nbox, classes):\n",
        "\n",
        "    y_min, y_max = b[1], b[3]\n",
        "    #y_min = min(nbox[0][1], nbox[1][1])\n",
        "    #y_max = max(nbox[0][3], nbox[1][3])\n",
        "    lower_limit = ((y_min + y_max) / 2) - (y_max - y_min) * 0.15\n",
        "    upper_limit = ((y_min + y_max) / 2) + (y_max - y_min) * 0.15\n",
        "    box_center_y_last = (nbox[-1][1] + nbox[-1][3])/2\n",
        "    box_center_y_last2 = (nbox[-2][1] + nbox[-2][3])/2\n",
        "\n",
        "    if (classes[-1] == 0) and (box_center_y_last > upper_limit):\n",
        "        if box_center_y_last2 > upper_limit:\n",
        "            classes[-2] += -1\n",
        "            classes[-1] = 9 #9.5\n",
        "        elif box_center_y_last2 < upper_limit:\n",
        "            classes[-1] = 9 #9.5\n",
        "\n",
        "    elif (classes[-1] == 9) and (box_center_y_last < lower_limit):\n",
        "        if box_center_y_last2 > upper_limit:\n",
        "            classes[-2] += -1\n",
        "            classes[-1] = 9 #9.5\n",
        "        elif box_center_y_last2 < upper_limit:\n",
        "            classes[-1] = 9 #9.5\n",
        "    else:\n",
        "        if box_center_y_last > upper_limit:\n",
        "            if classes[-1] == 0:\n",
        "                classes[-1] = 9 #9.5\n",
        "            else:\n",
        "                classes[-1] -= 1 #0.5\n",
        "        elif box_center_y_last < lower_limit:\n",
        "            classes[-1] += 1 #0.5\n",
        "    return classes"
      ],
      "metadata": {
        "id": "k4z9ZLtYdavq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_double(boxes, scores, classes):\n",
        "    pick = []\n",
        "    previous_box = [boxes[0], scores[0]]\n",
        "    for i, current_box in enumerate(zip(boxes, scores)):\n",
        "        if i == 0:\n",
        "            pick.append(i)\n",
        "            continue\n",
        "        x1, x2 = previous_box[0][0], previous_box[0][2]\n",
        "        xx1, xx2 = current_box[0][0], current_box[0][2]\n",
        "        overlap = max(max((xx2-x1), 0) - max((xx2-x2), 0) - max((xx1-x1), 0), 0) != 0\n",
        "\n",
        "        if overlap:\n",
        "            pick.pop()\n",
        "            if previous_box[1] > current_box[1]:\n",
        "                pick.append(i-1)\n",
        "            else:\n",
        "                pick.append(i)\n",
        "        else:\n",
        "            pick.append(i)\n",
        "        previous_box = current_box\n",
        "    return [boxes[j] for j in pick], [scores[j] for j in pick], [classes[j] for j in pick]"
      ],
      "metadata": {
        "id": "Jr6arQ94datH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NpEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        else:\n",
        "            return super(NpEncoder, self).default(obj)"
      ],
      "metadata": {
        "id": "iFB5UnLddarS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def horizontal_balance(img):\n",
        "\n",
        "    img_before = img\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img_gray = cv2.blur(img_gray, (3,3))\n",
        "    img_edges = cv2.Canny(img_gray, 100, 80, apertureSize=3)\n",
        "    lines = cv2.HoughLinesP(img_edges, 2, math.pi / 180.0, 50, minLineLength=70, maxLineGap=5)\n",
        "    lines_sorted = []\n",
        "\n",
        "    if lines is not None:\n",
        "        for [[x1, y1, x2, y2]] in lines:\n",
        "            if (x1 == x2):\n",
        "                continue\n",
        "            else:\n",
        "                lines_sorted.append([x1, y1, x2, y2])\n",
        "\n",
        "        angles = []\n",
        "\n",
        "        for [x1, y1, x2, y2] in lines_sorted:\n",
        "            angle = math.degrees(math.atan2(y2 - y1, x2 - x1))\n",
        "            if ((angle <= 50) and (1 < angle)) or ((-50<= angle) and ( angle < -1)):\n",
        "                angles.append(angle)\n",
        "        if len(angles) == 0:\n",
        "            img_rotated, median_angle = img, None\n",
        "        else:\n",
        "            median_angle = np.median(angles)\n",
        "            img_rotated = ndimage.rotate(img_before, median_angle)\n",
        "\n",
        "    else:\n",
        "        img_rotated, median_angle = img, None\n",
        "    return img_rotated, median_angle\n"
      ],
      "metadata": {
        "id": "utZmvbYjdapQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_text_file_by_line(path):\n",
        "    with open(path, mode='r', encoding='utf-8') as f:\n",
        "        for i in f:\n",
        "            i = i.strip()\n",
        "            if i:\n",
        "                yield i"
      ],
      "metadata": {
        "id": "GJWA_MkwdanS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_categories_file(csv_path):\n",
        "\n",
        "    result = {}\n",
        "    for line in read_text_file_by_line(csv_path):\n",
        "        split = line.split(',')\n",
        "        if len(split) < 2 or any(not i for i in split):\n",
        "            continue\n",
        "        result[split[0]] = int(split[1])\n",
        "    return result"
      ],
      "metadata": {
        "id": "5KM_xU6fdalJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_np_array(x, column=None, flip=False):\n",
        "    x = x[np.argsort(x[:, column])]\n",
        "    if flip:\n",
        "        x = np.flip(x, axis=0)\n",
        "    return x"
      ],
      "metadata": {
        "id": "yYgMbf8IdajV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_classification_model(IM_SHAPE, NC):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Block 1\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=L2(l2=0.01), input_shape=IM_SHAPE))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=L2(l2=0.01)))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=L2(l2=0.01)))\n",
        "    model.add(MaxPool2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Block 2\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=L2(l2=0.01)))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=L2(l2=0.01)))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=L2(l2=0.01)))\n",
        "    model.add(MaxPool2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    # Block 3\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=L2(l2=0.01)))\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=L2(l2=0.01)))\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=L2(l2=0.01)))\n",
        "    model.add(MaxPool2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Block 4\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=L2(l2=0.01)))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=L2(l2=0.01)))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same', kernel_regularizer=L2(l2=0.01)))\n",
        "    model.add(MaxPool2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Dense\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Softmax\n",
        "    model.add(Dense(NC, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "8xCguD1ndahP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MeterDetection:\n",
        "\n",
        "    def __init__(self, meter_type):\n",
        "\n",
        "        isCuda = torch.cuda.is_available()\n",
        "        print(f'cuda is_available:{torch.cuda.is_available()}')\n",
        "        torch.cuda.device(0)\n",
        "\n",
        "        if isCuda:\n",
        "            self.device = torch.device('cuda')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "\n",
        "        self.input_size = 320\n",
        "\n",
        "        # define model\n",
        "\n",
        "        self.model_yolo = torch.hub.load(\n",
        "            \"ultralytics/yolov5\",\n",
        "            \"custom\",\n",
        "            path=f\"./weights/{meter_type}_box.pt\",\n",
        "            force_reload = True,\n",
        "        )\n",
        "        if isCuda:\n",
        "            self.model_yolo = self.model_yolo.cuda()\n",
        "\n",
        "        self.model_yolo = self.model_yolo.autoshape()\n",
        "\n",
        "\n",
        "\n",
        "    def detect(self, x, threshold=0.7):\n",
        "\n",
        "        self.model_yolo.conf = threshold\n",
        "        x = cv2.cvtColor(np.float32(x), cv2.COLOR_BGR2RGB)\n",
        "        results = self.model_yolo([x], size=self.input_size)\n",
        "        results = results.xyxy[0].cpu().numpy()\n",
        "        boxes = results[:, :4]\n",
        "        scores = results[:, 4]\n",
        "        classes = results[:, 5]\n",
        "\n",
        "        return boxes, scores, classes"
      ],
      "metadata": {
        "id": "Av5movtZdafG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NumberDetection:\n",
        "\n",
        "    def __init__(self, meter_type):\n",
        "        isCuda = torch.cuda.is_available()\n",
        "        print(f'cuda is_available:{torch.cuda.is_available()}')\n",
        "        torch.cuda.device(0)\n",
        "        if isCuda:\n",
        "            self.device = torch.device('cuda')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "        self.input_size = 320\n",
        "\n",
        "        # define model\n",
        "        self.model_yolo = torch.hub.load(\n",
        "            \"ultralytics/yolov5\",\n",
        "            \"custom\",\n",
        "            path=f\"./weights/{meter_type}_num.pt\",\n",
        "            force_reload = True,\n",
        "        )\n",
        "        if isCuda:\n",
        "            self.model_yolo = self.model_yolo.cuda()\n",
        "        self.model_yolo = self.model_yolo.autoshape()\n",
        "\n",
        "    def detect(self, x, threshold=0.7):\n",
        "        self.model_yolo.conf = threshold\n",
        "        x = cv2.cvtColor(np.float32(x), cv2.COLOR_BGR2RGB)\n",
        "        results = self.model_yolo([x], size=self.input_size)\n",
        "        results = results.xyxy[0].cpu().numpy()\n",
        "\n",
        "        boxes = results[:, :4]\n",
        "        scores = results[:, 4]\n",
        "        classes = results[:, 5]\n",
        "\n",
        "        return boxes, scores, classes"
      ],
      "metadata": {
        "id": "Mq7kyBLtdadH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_model():\n",
        "\n",
        "    current_time = time.time()\n",
        "    global ND, MD, NumberC\n",
        "    ND = {}\n",
        "    MD = {}\n",
        "    NumberC = {}\n",
        "\n",
        "    type_list = [1, 2, 4, 5, 6, 9]\n",
        "    # type_list = [9]\n",
        "\n",
        "    IM_SHAPE = (44, 27, 3)\n",
        "    NC = 10\n",
        "\n",
        "\n",
        "    for type_ in type_list:\n",
        "        print(f'start load model of type: {type_}')\n",
        "        ND[type_] = NumberDetection(type_)\n",
        "        MD[type_] = MeterDetection(type_)\n",
        "\n",
        "        model = make_classification_model(IM_SHAPE, NC)\n",
        "        model.load_weights(f'./weights/{type_}_classification.h5')\n",
        "        NumberC[type_] = model\n",
        "\n",
        "    print(f'Models loading time = {time.time() - current_time}')"
      ],
      "metadata": {
        "id": "4u5Yu-d0dabR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_number2(img,  pure_img_name, meter_type=0, box=None, rot_angle=None):\n",
        "    ''' image should be numpy array, box list of x1,y1,x2,y2, rot_angle[degree]'''\n",
        "\n",
        "    if MD.get(meter_type) is None:\n",
        "        meter_type = 0\n",
        "\n",
        "    frmt_date = dt.datetime.utcfromtimestamp(\n",
        "        time.time()).strftime(\"%Y%m%d %H:%M\")\n",
        "\n",
        "    res_dict = {}\n",
        "    res_dict[\"name\"] = pure_img_name\n",
        "    res_dict[\"DateTime\"] = frmt_date[:8] # 20220901\n",
        "    res_dict[\"Image_height\"] = img.shape[0]\n",
        "    res_dict[\"Image_width\"] = img.shape[1]\n",
        "\n",
        "    result = []\n",
        "\n",
        "    if rot_angle is None:\n",
        "        img, ang = horizontal_balance(img)\n",
        "        if ang is None:\n",
        "            res_dict[\"Rotation_angle\"] = 0\n",
        "        else:\n",
        "            res_dict[\"Rotation_angle\"] = ang\n",
        "        print('rotation compute complete')\n",
        "    else:\n",
        "        img = ndimage.rotate(img, rot_angle)\n",
        "        res_dict[\"Rotation_angle\"] = rot_angle\n",
        "\n",
        "    draw = img\n",
        "    h, w, _ = img.shape\n",
        "\n",
        "    if box is None:\n",
        "        boxes, scores, labels = MD[meter_type].detect(img, 0.5)\n",
        "\n",
        "        if len(scores) != 0:\n",
        "            idx = np.argmax(scores)\n",
        "            box = boxes[idx]\n",
        "\n",
        "            num_patches = []\n",
        "\n",
        "            b = box.astype(int)\n",
        "            res_dict[\"box\"] = b\n",
        "\n",
        "            cropped = draw[b[1]:b[3], b[0]:b[2]]\n",
        "            current_time = time.time()\n",
        "            nboxes,_,_ = ND[meter_type].detect(cropped, 0.6)\n",
        "\n",
        "            if len(nboxes) <= 2:\n",
        "                print(f'no number is detected for {pure_img_name}')\n",
        "            else:\n",
        "                nboxes = nboxes.astype(int)\n",
        "                nboxes = sort_np_array(nboxes, column=0, flip=False)\n",
        "\n",
        "                for box in nboxes:\n",
        "                    cropped_number = cropped[ box[1]:box[3], box[0]:box[2] ]\n",
        "                    num_patches.append(cropped_number)\n",
        "\n",
        "                current_time = time.time()\n",
        "                #num_detected는 check_double후에 처리.\n",
        "                #res_dict[\"num_detected\"] = len(num_patches)\n",
        "                res_dict[\"objects_info\"] = []\n",
        "\n",
        "                class_res = []\n",
        "\n",
        "                score_res = []\n",
        "\n",
        "\n",
        "\n",
        "                for patch, box in zip(num_patches, nboxes):\n",
        "\n",
        "                    patch_resized = cv2.resize(patch, (27, 44))\n",
        "                    patch_resized = patch_resized[np.newaxis,...]\n",
        "                    predictions = NumberC[meter_type].predict(patch_resized)\n",
        "                    result = np.argmax(predictions)\n",
        "                    score = np.max(predictions)\n",
        "                    class_res.append(result)\n",
        "                    score_res.append(score)\n",
        "\n",
        "                nboxes, score_res, class_res = check_double(nboxes, score_res, class_res)\n",
        "\n",
        "                if len(nboxes) <= 2:\n",
        "                    print(f'filtered len nboxes : {len(nboxes)} {pure_img_name}')\n",
        "                    return 0\n",
        "\n",
        "                res_dict[\"num_detected\"] = len(nboxes)\n",
        "                class_res = decimal_process(box, nboxes, class_res)\n",
        "\n",
        "                for box, result, score in zip(nboxes, class_res, score_res):\n",
        "                    temp_dict = {}\n",
        "                    # cv2.rectangle(draw, (box[0]+b[0], box[1]+b[1]), (box[2]+b[0], box[3]+b[1]), (0,0,255), 2)\n",
        "                    temp_dict[\"class\"] = result\n",
        "                    temp_dict[\"coordinate\"] = list(box)\n",
        "                    temp_dict[\"probability\"] = np.round(score,5)\n",
        "                    res_dict[\"objects_info\"].append(temp_dict)\n",
        "    else:\n",
        "        print(f'no box is detected for {pure_img_name}')\n",
        "\n",
        "    return json.dumps(res_dict, indent=4, cls=NpEncoder)"
      ],
      "metadata": {
        "id": "MBk6QoixdaZA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_number(img_path, meter_type, box=None, rot_angle=None):\n",
        "    ''' image should be numpy array, box list of x1,y1,x2,y2, rot_angle[degree]'''\n",
        "\n",
        "    print(f\"loading image from : {img_path}\")\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    pure_img_name = os.path.split(img_path)[-1]\n",
        "\n",
        "    frmt_date = dt.datetime.utcfromtimestamp(\n",
        "        time.time()).strftime(\"%Y%m%d %H:%M\")\n",
        "\n",
        "    jsonDump = detect_number2(img, pure_img_name, meter_type, box, rot_angle)\n",
        "\n",
        "    res_json_path = os.path.join(os.path.dirname(\n",
        "        img_path), f'{frmt_date[:8]}_{pure_img_name.split(\".\")[0]}.json')\n",
        "\n",
        "    print(f'json file saved to {res_json_path}')\n",
        "\n",
        "    with open(res_json_path, \"w\") as json_file:\n",
        "        json_file.write(jsonDump)\n",
        "\n",
        "    resultjson = json.loads(jsonDump)\n",
        "\n",
        "    csvrow = []\n",
        "    csvrow.append(resultjson[\"name\"])\n",
        "    strval = ''\n",
        "\n",
        "    if resultjson.get(\"objects_info\") is not None:\n",
        "        for info in resultjson[\"objects_info\"]:\n",
        "            strval += str(info[\"class\"])\n",
        "\n",
        "        csvrow.append(strval)\n",
        "\n",
        "        for info in resultjson[\"objects_info\"]:\n",
        "            csvrow.append(str(info[\"probability\"]))\n",
        "\n",
        "    with open('result.csv', 'a', newline='') as resultfile:\n",
        "        wr = csv.writer(resultfile)\n",
        "        wr.writerow(csvrow)\n",
        "\n",
        "    return jsonDump\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # img = cv2.imread('./54.jpg')\n",
        "    # cv2.imshow('',img)\n",
        "    # cv2.waitKey(0)\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    load_all_model()\n",
        "    img_path = './dataset/test_data/8.jpg'\n",
        "    detect_number(img_path)\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    load_all_model()\n",
        "    IN_IMAGES_PATH = 'dataset/type-9'\n",
        "    # IN_IMAGES_PATH = 'E:/DM/PROJECTS/HY.CHECK/imageprocessing/20210608-test/'\n",
        "\n",
        "    print(f'Reading images from {IN_IMAGES_PATH}')\n",
        "    for root, dirs, files in os.walk(IN_IMAGES_PATH):\n",
        "        for filename in files:\n",
        "            if os.path.splitext(filename)[-1].lower() not in ['.jpg']:\n",
        "                continue\n",
        "            detect_number(os.path.join(root, filename), 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ffO_x_NFdaXB",
        "outputId": "3c5fa63c-c722-4513-d462-37ab6abc4aff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start load model of type: 1\n",
            "cuda is_available:False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  \"You are about to download and run code from an untrusted repository. In a future release, this won't \"\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"ipython\" not found, attempting AutoUpdate...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (7.9.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython) (2.0.10)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython) (0.7.0)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.18.1\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "YOLOv5 🚀 2022-10-24 Python-3.7.15 torch-1.12.1+cu113 CPU\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDetectMultiBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# detection model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mautoshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weights, device, dnn, data, fp16, fuse)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# model stride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py\u001b[0m in \u001b[0;36mattempt_load\u001b[0;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempt_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ema'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FP32 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weights/1_num.pt'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# arbitrary model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py\u001b[0m in \u001b[0;36mattempt_load\u001b[0;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempt_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ema'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FP32 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weights/1_num.pt'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1ecf7a760a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m#########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mload_all_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./dataset/test_data/8.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mdetect_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-5fafb8380697>\u001b[0m in \u001b[0;36mload_all_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtype_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'start load model of type: {type_}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mND\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNumberDetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mMD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeterDetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-46ad4ae1ba48>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, meter_type)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;34m\"custom\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"./weights/{meter_type}_num.pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mforce_reload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misCuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m                                            verbose=verbose, skip_validation=skip_validation)\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\u001b[0m in \u001b[0;36mcustom\u001b[0;34m(path, autoshape, _verbose, device)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcustom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'path/to/model.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# YOLOv5 custom or local model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_verbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mhelp_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://github.com/ultralytics/yolov5/issues/36'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{e}. Cache may be out of date, try `force_reload=True` or see {help_url} for help.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: [Errno 2] No such file or directory: 'weights/1_num.pt'. Cache may be out of date, try `force_reload=True` or see https://github.com/ultralytics/yolov5/issues/36 for help."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cpmeLIUUdaU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JnErcCxDdaSj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}